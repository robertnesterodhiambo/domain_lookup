# ğŸŒ Domain Intelligence Platform

A scalable system that ingests, processes, and analyzes domain data â€” including WHOIS info, DNS records, website availability, accessibility, and subdomain counts. Supports flexible filtering through a web-based frontend.

---

## ğŸ“¦ Features

- âœ… **350M+ domains** in database (with 1.2M+ daily updates)
- ğŸ—‚ï¸ **WHOIS** and **NSLOOKUP** lookup for every domain
- ğŸ”„ Tracks **new and deleted** domains daily
- ğŸŒ **Geolocation** from WHOIS and web hosting data
- ğŸ•·ï¸ Crawl websites and count number of pages (via sitemap or free crawlers)
- â™¿ **Accessibility check** (via open source tools)
- ğŸŒ Count **subdomains** using open-source GitHub tools
- ğŸ§ª Filter and search using:
  - TLD (Top-Level Domain)
  - Country from WHOIS
  - Country from Web Host
  - Page Count
  - Subdomain Count
  - Accessibility Score
- ğŸ” Supports AND / OR filters, and single/multi-criteria filtering
- ğŸ–¥ï¸ Simple Web Frontend with powerful search

---

## ğŸ—ï¸ Architecture Overview

1. **Downloader**
   - Downloads domain data and deletions (daily).
   - URL provided for regular updates.

2. **PostgreSQL Database**
   - Stores all domain metadata.
   - Indexed for fast filtering.

3. **Processing Pipeline**
   - WHOIS lookup (`whois domain.com`)
   - NSLOOKUP (`nslookup domain.com`)
   - Sitemap crawling & page count
   - Accessibility score (via free GitHub tools)
   - Subdomain enumeration

4. **Web Frontend**
   - Filter interface using TLD, location, counts, and accessibility score.
   - Responsive and minimal UI.

---

## ğŸ› ï¸ Technologies Used

- **PostgreSQL** â€“ Scalable relational database.
- **Python** â€“ Processing scripts and backend.
- **Selenium, Requests, BeautifulSoup** â€“ Crawling.
- **whois, nslookup** â€“ Standard Ubuntu commands.
- **subfinder, amass, assetfinder** â€“ Open-source subdomain finders.
- **Pa11y, Lighthouse CLI** â€“ Accessibility checking.
- **React or Flask with HTML/CSS** â€“ Frontend (configurable).

---

## ğŸ“ Folder Structure


